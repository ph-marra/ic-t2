C:\Users\ph_ma\OneDrive\UFU\2023-2\IC\trabalho2>julia metrics.jl

-----------------------------------------------
Chain(Conv((3, 3), 3 => 64, relu, pad=1), BatchNorm(64), Conv((3, 3), 64 => 64, relu, pad=1), BatchNorm(64), MaxPool((2, 2)), Conv((3, 3), 64 => 128, relu, pad=1), BatchNorm(128), Conv((3, 3), 128 => 128, relu, pad=1), BatchNorm(128), MaxPool((2, 2)), Conv((3, 3), 128 => 256, relu, pad=1), BatchNorm(256), Conv((3, 3), 256 => 256, relu, pad=1), BatchNorm(256), Conv((3, 3), 256 => 256, relu, pad=1), BatchNorm(256), Conv((3, 3), 256 => 256, relu, pad=1), MaxPool((2, 2)), Conv((3, 3), 256 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), MaxPool((2, 2)), Conv((3, 3), 512 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), BatchNorm(512), Conv((3, 3), 512 => 512, relu, pad=1), MaxPool((2, 2)), #3, Dense(512 => 4096, relu), Dropout(0.5), Dense(4096 => 4096, relu), Dropout(0.5), Dense(4096 => 10), softmax)
-----------------------------------------------
A ConfusionMatrix BetaMLModel (fitted)

-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):

11×11 Matrix{Any}:
 "Labels"     "3"     "8"     "0"     "6"     "1"     "9"     "5"     "7"     "4"     "2"
 "3"       740       5      12      40       2       3      92      20      33      53
 "8"        14     893      58       3       7      17       0       3       1       4
 "0"        11      36     871       3       4      20       0       4       9      42
 "6"        68       3       4     867       0       0       1       0      21      36
 "1"         7      15      11       1     906      59       0       0       1       0
 "9"        11      13      26       1      32     908       1       7       0       1
 "5"       215       1      10       7       1       1     674      37      32      22
 "7"        49       1      15       0       0       7      36     845      38       9
 "4"        48       1      12      26       1       0      20      25     814      53
 "2"        76       1      59      36       0       2      17       8      58     743
Normalised scores actual (rows) vs predicted (columns):

11×11 Matrix{Any}:
 "Labels"   "3"    "8"    "0"    "6"    "1"    "9"    "5"    "7"    "4"    "2"
 "3"       0.74   0.005  0.012  0.04   0.002  0.003  0.092  0.02   0.033  0.053
 "8"       0.014  0.893  0.058  0.003  0.007  0.017  0.0    0.003  0.001  0.004
 "0"       0.011  0.036  0.871  0.003  0.004  0.02   0.0    0.004  0.009  0.042
 "6"       0.068  0.003  0.004  0.867  0.0    0.0    0.001  0.0    0.021  0.036
 "1"       0.007  0.015  0.011  0.001  0.906  0.059  0.0    0.0    0.001  0.0
 "9"       0.011  0.013  0.026  0.001  0.032  0.908  0.001  0.007  0.0    0.001
 "5"       0.215  0.001  0.01   0.007  0.001  0.001  0.674  0.037  0.032  0.022
 "7"       0.049  0.001  0.015  0.0    0.0    0.007  0.036  0.845  0.038  0.009
 "4"       0.048  0.001  0.012  0.026  0.001  0.0    0.02   0.025  0.814  0.053
 "2"       0.076  0.001  0.059  0.036  0.0    0.002  0.017  0.008  0.058  0.743

 *** CONFUSION REPORT ***

- Accuracy:               0.8261
- Misclassification rate: 0.17390000000000005
- Number of classes:      10

  N Class   precision   recall  specificity  f1score  actual_count  predicted_count
                          TPR       TNR                 support

  1 3           0.597    0.740        0.945    0.661         1000            1239
  2 8           0.922    0.893        0.992    0.907         1000             969
  3 0           0.808    0.871        0.977    0.838         1000            1078
  4 6           0.881    0.867        0.987    0.874         1000             984
  5 1           0.951    0.906        0.995    0.928         1000             953
  6 9           0.893    0.908        0.988    0.900         1000            1017
  7 5           0.801    0.674        0.981    0.732         1000             841
  8 7           0.890    0.845        0.988    0.867         1000             949
  9 4           0.808    0.814        0.979    0.811         1000            1007
 10 2           0.772    0.743        0.976    0.757         1000             963

- Simple   avg.    0.832    0.826        0.981    0.828
- Weigthed avg.    0.832    0.826        0.981    0.828

-----------------------------------------------------------------
Output of `info(cm)`:
- mean_precision:       (0.8323130547293663, 0.8323130547293663)
- fitted_records:       10000
- specificity:  [0.9445555555555556, 0.9915555555555555, 0.977, 0.987, 0.9947777777777778, 0.9878888888888889, 0.9814444444444445, 0.9884444444444445, 0.9785555555555555, 0.9755555555555555]
- precision:    [0.5972558514931396, 0.9215686274509803, 0.8079777365491652, 0.8810975609756098, 0.950682056663169, 0.8928220255653884, 0.8014268727705113, 0.8904109589041096, 0.8083416087388282, 0.7715472481827622]
- misclassification:    0.17390000000000005
- mean_recall:  (0.8261000000000001, 0.8261)
- n_categories: 10
- normalised_scores:    [0.74 0.005 0.012 0.04 0.002 0.003 0.092 0.02 0.033 0.053; 0.014 0.893 0.058 0.003 0.007 0.017 0.0 0.003 0.001 0.004; 0.011 0.036 0.871 0.003 0.004 0.02 0.0 0.004 0.009 0.042; 0.068 0.003 0.004 0.867 0.0 0.0 0.001 0.0 0.021 0.036; 0.007 0.015 0.011 0.001 0.906 0.059 0.0 0.0 0.001 0.0; 0.011 0.013 0.026 0.001 0.032 0.908 0.001 0.007 0.0 0.001; 0.215 0.001 0.01 0.007 0.001 0.001 0.674 0.037 0.032 0.022; 0.049 0.001 0.015 0.0 0.0 0.007 0.036 0.845 0.038 0.009; 0.048 0.001 0.012 0.026 0.001 0.0 0.02 0.025 0.814 0.053; 0.076 0.001 0.059 0.036 0.0 0.002 0.017 0.008 0.058 0.743]
- tn:   [8501, 8924, 8793, 8883, 8953, 8891, 8833, 8896, 8807, 8780]
- mean_f1score: (0.8276004844423532, 0.8276004844423532)
- actual_count: [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]
- accuracy:     0.8261
- recall:       [0.74, 0.893, 0.871, 0.867, 0.906, 0.908, 0.674, 0.845, 0.814, 0.743]
- f1score:      [0.6610093791871371, 0.9070594210259014, 0.8383060635226179, 0.873991935483871, 0.9278033794162827, 0.9003470500743679, 0.7322107550244432, 0.8671113391482812, 0.8111609367214748, 0.7570045848191543]
- mean_specificity:     (0.9806777777777779, 0.9806777777777775)
- predicted_count:      [1239, 969, 1078, 984, 953, 1017, 841, 949, 1007, 963]
- scores:       [740 5 12 40 2 3 92 20 33 53; 14 893 58 3 7 17 0 3 1 4; 11 36 871 3 4 20 0 4 9 42; 68 3 4 867 0 0 1 0 21 36; 7 15 11 1 906 59 0 0 1 0; 11 13 26 1 32 908 1 7 0 1; 215 1 10 7 1 1 674 37 32 22; 49 1 15 0 0 7 36 845 38 9; 48 1 12 26 1 0 20 25 814 53; 76 1 59 36 0 2 17 8 58 743]
- tp:   [740, 893, 871, 867, 906, 908, 674, 845, 814, 743]
- fn:   [260, 107, 129, 133, 94, 92, 326, 155, 186, 257]
- categories:   [3, 8, 0, 6, 1, 9, 5, 7, 4, 2]
- fp:   [499, 76, 207, 117, 47, 109, 167, 104, 193, 220]

-----------------------------------------------
Dict{String, Any}("mean_precision" => (0.8323130547293663, 0.8323130547293663), "fitted_records" => 10000, "specificity" => [0.9445555555555556, 0.9915555555555555, 0.977, 0.987, 0.9947777777777778, 0.9878888888888889, 0.9814444444444445, 0.9884444444444445, 0.9785555555555555, 0.9755555555555555], "precision" => [0.5972558514931396, 0.9215686274509803, 0.8079777365491652, 0.8810975609756098, 0.950682056663169, 0.8928220255653884, 0.8014268727705113, 0.8904109589041096, 0.8083416087388282, 0.7715472481827622], "misclassification" => 0.17390000000000005, "mean_recall" => (0.8261000000000001, 0.8261), "n_categories" => 10, "normalised_scores" => [0.74 0.005 0.012 0.04 0.002 0.003 0.092 0.02 0.033 0.053; 0.014 0.893 0.058 0.003 0.007 0.017 0.0 0.003 0.001 0.004; 0.011 0.036 0.871 0.003 0.004 0.02 0.0 0.004 0.009 0.042; 0.068 0.003 0.004 0.867 0.0 0.0 0.001 0.0 0.021 0.036; 0.007 0.015 0.011 0.001 0.906 0.059 0.0 0.0 0.001 0.0; 0.011 0.013 0.026 0.001 0.032 0.908 0.001 0.007 0.0 0.001; 0.215 0.001 0.01 0.007 0.001 0.001 0.674 0.037 0.032 0.022; 0.049 0.001 0.015 0.0 0.0 0.007 0.036 0.845 0.038 0.009; 0.048 0.001 0.012 0.026 0.001 0.0 0.02 0.025 0.814 0.053; 0.076 0.001 0.059 0.036 0.0 0.002 0.017 0.008 0.058 0.743], "tn" => [8501, 8924, 8793, 8883, 8953, 8891, 8833, 8896, 8807, 8780], "mean_f1score" => (0.8276004844423532, 0.8276004844423532), "actual_count" => [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000], "accuracy" => 0.8261, "recall" => [0.74, 0.893, 0.871, 0.867, 0.906, 0.908, 0.674, 0.845, 0.814, 0.743], "f1score" => [0.6610093791871371, 0.9070594210259014, 0.8383060635226179, 0.873991935483871, 0.9278033794162827, 0.9003470500743679, 0.7322107550244432, 0.8671113391482812, 0.8111609367214748, 0.7570045848191543], "mean_specificity" => (0.9806777777777779, 0.9806777777777775), "predicted_count" => [1239, 969, 1078, 984, 953, 1017, 841, 949, 1007, 963], "scores" => [740 5 12 40 2 3 92 20 33 53; 14 893 58 3 7 17 0 3 1 4; 11 36 871 3 4 20 0 4 9 42; 68 3 4 867 0 0 1 0 21 36; 7 15 11 1 906 59 0 0 1 0; 11 13 26 1 32 908 1 7 0 1; 215 1 10 7 1 1 674 37 32 22; 49 1 15 0 0 7 36 845 38 9; 48 1 12 26 1 0 20 25 814 53; 76 1 59 36 0 2 17 8 58 743], "tp" => [740, 893, 871, 867, 906, 908, 674, 845, 814, 743], "fn" => [260, 107, 129, 133, 94, 92, 326, 155, 186, 257], "categories" => [3, 8, 0, 6, 1, 9, 5, 7, 4, 2], "fp" => [499, 76, 207, 117, 47, 109, 167, 104, 193, 220])
-----------------------------------------------